{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import raw data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Athens Greece Baghdad Iraq\n",
      "Athens Greece Bangkok Thailand\n",
      "Athens Greece Beijing China\n",
      "Athens Greece Berlin Germany\n",
      "Athens Greece Bern Switzerland\n",
      "['Athens Greece Baghdad Iraq', 'Athens Greece Bangkok Thailand', 'Athens Greece Beijing China', 'Athens Greece Berlin Germany', 'Athens Greece Bern Switzerland']\n"
     ]
    }
   ],
   "source": [
    "#-*- utf-8 -*-\n",
    "import codecs,re\n",
    "sentece=[]\n",
    "#r1=open(\"D:/iii/w2v/sampleU.txt\",\"r\")\n",
    "text = codecs.open(\"D:/iii/w2v/sample.txt\",\"r\",\"utf8\")\n",
    "for n in text.readlines():\n",
    "    n2=re.sub(\"\\r\\n\",\"\",n)    \n",
    "    sentece.append(n2.encode('utf8'))\n",
    "    #sentece.append(nn)\n",
    "\n",
    "for n in sentece[:5]:\n",
    "    print n\n",
    "\n",
    "print sentece[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "#for word2vec C format:\n",
    "#model = gensim.models.Word2Vec.load_word2vec_format('D:/iii/w2v/sample.txt', binary=False) \n",
    "mymodel = gensim.models.Word2Vec([s.split() for s in sentece], min_count=2,size=2,workers=4)\n",
    "model.save('D:/iii/w2v/mymodel')\n",
    "#model = Word2Vec.load(\"D:/iii/w2v/mymodel\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print out elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['precise', 'taller', 'Nashville', 'unimpressive', 'Baltimore', 'Arlington', 'queen', 'cheap', 'Minnesota', 'competitive', 'Illinois', 'cheerful', 'bright', 'efficient', 'inconsistent', 'precisely', 'Indiana', 'cold', 'Amarillo', 'typical', 'prince', 'Texas', 'reluctant', 'shorter', 'rapid', 'rapidly', 'Tulsa', 'Sacramento', 'Kansas', 'father', 'young', 'uninformative', 'brighter', 'unpleasant', 'easy', 'Atlanta', 'heavier', 'Oklahoma', 'Tampa', 'niece', 'dad', 'his', 'rational', 'wide', 'Miami', 'big', 'professional', 'safe', 'impossible', 'Lexington', 'cheaper', 'possibly', 'complete', 'sons', 'mom', 'immediately', 'unethical', 'bigger', 'cool', 'infrequent', 'impressive', 'Fresno', 'cooler', 'policeman', 'Washington', 'brother', 'Minneapolis', 'grandfather', 'large', 'informative', 'bad', 'Milwaukee', 'she', 'reasonable', 'stepbrother', 'quick', 'Seattle', 'husband', 'heavy', 'distasteful', 'calmly', 'uncompetitive', 'unproductive', 'tougher', 'Fontana', 'colder', 'consistent', 'hard', 'simple', 'Denver', 'likely', 'louder', 'warmer', 'Oxnard', 'uncomfortable', 'nephew', 'Maryland', 'stepson', 'lower', 'brothers', 'faster', 'Arizona', 'Wisconsin', 'illogical', 'Pittsburgh', 'Michigan', 'Oregon', 'grandmother', 'seriously', 'Massachusetts', 'undecided', 'grandpa', 'new', 'stepdaughter', 'Houston', 'swiftly', 'unsure', 'Wichita', 'weak', 'Cleveland', 'Florida', 'free', 'pleasant', 'small', 'Tucson', 'slow', 'grandma', 'Ohio', 'safer', 'strong', 'swift', 'princess', 'simpler', 'boy', 'sister', 'daughter', 'her', 'smarter', 'most', 'freely', 'larger', 'completely', 'honest', 'happily', 'infrequently', 'unaware', 'amazingly', 'usually', 'Tacoma', 'safely', 'unconvincing', 'quietly', 'unclear', 'tough', 'impossibly', 'Dallas', 'suddenly', 'irresponsible', 'logical', 'son', 'Colorado', 'amazing', 'Louisville', 'quicker', 'fast', 'wider', 'slower', 'rarely', 'cheerfully', 'old', 'Indianapolis', 'smaller', 'sharper', 'unknown', 'informed', 'long', 'quickly', 'quiet', 'slowly', 'calm', 'unfortunately', 'Akron', 'California', 'Mesa', 'man', 'Memphis', 'inefficient', 'happy', 'Georgia', 'deeper', 'mostly', 'uninformed', 'Pennsylvania', 'luckily', 'acceptable', 'apparent', 'great', 'Detroit', 'Athens', 'Anaheim', 'warm', 'uncle', 'newer', 'known', 'rare', 'Austin', 'inconvenient', 'he', 'king', 'woman', 'uncertain', 'Stockton', 'Oakland', 'wife', 'harder', 'Worcester', 'professionally', 'easier', 'bride', 'decided', 'obvious', 'smart', 'Boston', 'stepmother', 'stepfather', 'weaker', 'hotter', 'Chicago', 'certain', 'Philadelphia', 'deep', 'comfortable', 'high', 'good', 'unlikely', 'sharp', 'Omaha', 'apparently', 'girl', 'worse', 'greater', 'Glendale', 'productive', 'loud', 'Phoenix', 'fortunately', 'convenient', 'responsible', 'sudden', 'fortunate', 'occasional', 'tight', 'hot', 'occasionally', 'low', 'unfortunate', 'better', 'Jacksonville', 'daughters', 'higher', 'Bakersfield', 'sure', 'irrational', 'tasteful', 'Greece', 'stepsister', 'granddaughter', 'convincing', 'Hawaii', 'Kentucky', 'possible', 'reluctantly', 'ethical', 'Nebraska', 'immediate', 'tall', 'sisters', 'Alabama', 'aware', 'unacceptable', 'tighter', 'efficiently', 'dishonest', 'younger', 'short', 'Tallahassee', 'longer', 'grandson', 'older', 'furiously', 'clear', 'groom', 'stronger', 'unreasonable', 'lucky', 'obviously', 'Huntsville', 'Tennessee', 'aunt', 'Honolulu', 'mother', 'furious', 'serious', 'Portland', 'policewoman', 'typically', 'usual']\n"
     ]
    }
   ],
   "source": [
    "vocab = list(mymodel.vocab.keys())\n",
    "print vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity between policewoman and usual is -0.98946981905\n",
      "[('stepsister', 0.9999998807907104), ('stepfather', 0.9999825954437256), ('mother', 0.9998492002487183), ('he', 0.9997529983520508), ('brothers', 0.9994999170303345), ('sisters', 0.999199628829956), ('dad', 0.9983676075935364), ('prince', 0.9982269406318665), ('niece', 0.9969794154167175), ('sons', 0.9966608881950378)]\n"
     ]
    }
   ],
   "source": [
    "print \"similarity between policewoman and usual is \" + str(mymodel.similarity(\"policewoman\", \"usual\"))\n",
    "print mymodel.most_similar(\"policewoman\")\n",
    "#model.similarity('Athens Greece Cairo Egypt','Baghdad Iraq Hanoi Vietnam')  #error\n",
    "#model.doesnt_match(\"Baghdad Iraq Havana\") #meaning\n",
    "#model['Baghdad Iraq Havana Cuba'] #???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['second', 'first', 'third', 'sentence']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.045047222198946593"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "sentences = [('first', 'sentence'), ('second', 'sentence'),('third', 'sentence')]\n",
    "\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)\n",
    "vocab = list(model.vocab.keys())\n",
    "print vocab\n",
    "model.similarity('first', 'second')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dir(model):\n",
    "    ['__class__', '__contains__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', \n",
    "     '__getitem__', '__hash__', '__ignoreds', '__init__', '__module__', '__new__', '__numpys', \n",
    "     '__recursive_saveloads', '__reduce__', '__reduce_ex__', '__repr__', '__scipys', '__setattr__', \n",
    "     '__sizeof__', '__slotnames__', '__str__', '__subclasshook__', '__weakref__', '_adapt_by_suffix', \n",
    "     '_do_train_job', '_load_specials', '_raw_word_count', '_save_specials', '_score_job_words', \n",
    "     '_smart_save', 'accuracy', 'alpha', 'build_vocab', 'cbow_mean', 'clear_sims', 'corpus_count', \n",
    "     'create_binary_tree', 'cum_table', 'doesnt_match', 'estimate_memory', 'finalize_vocab', 'hashfxn', \n",
    "     'hs', 'index2word', 'init_sims', 'intersect_word2vec_format', 'iter', 'layer1_size', 'load', \n",
    "     'load_word2vec_format', 'log_accuracy', 'make_cum_table', 'max_vocab_size', 'min_alpha', 'min_count', \n",
    "     'most_similar', 'most_similar_cosmul', 'n_similarity', 'neg_labels', 'negative', 'null_word', \n",
    "     'random', 'raw_vocab', 'reset_from', 'reset_weights', 'sample', 'save', 'save_word2vec_format', \n",
    "     'scale_vocab', 'scan_vocab', 'score', 'seed', 'seeded_vector', 'sg', 'similarity', 'syn0', \n",
    "     'syn0_lockf', 'syn0norm', 'syn1', 'total_train_time', 'train', 'train_count', 'vector_size', \n",
    "     'vocab', 'window', 'workers']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
